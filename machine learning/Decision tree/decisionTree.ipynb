{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d7d3e3",
   "metadata": {},
   "source": [
    "<h2>1. Introduction to Decision tree algo.<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d38ae2",
   "metadata": {},
   "source": [
    "- uses tree like structure with possible combinations to solve a particular problem.\n",
    "- belongs to class of supervised learning and used for both classification and regression purpose.\n",
    "- structure includes a root node, branches, and leaf nodes.\n",
    "- A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e2c43",
   "metadata": {},
   "source": [
    "We make some assumptions while implementing the Decision-Tree algorithm. These are listed below:-\n",
    "\n",
    "1. At the beginning, the whole training set is considered as the root.\n",
    "2. Feature values need to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "3. Records are distributed recursively on the basis of attribute values.\n",
    "4. Order to placing attributes as root or internal node of the tree is done by using some statistical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec7246",
   "metadata": {},
   "source": [
    "<h4> 2. classification and regression tree(CART)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7ee19",
   "metadata": {},
   "source": [
    "- decision tree is also known as <B>CART</B><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabafb1",
   "metadata": {},
   "source": [
    "<h4> 3. Decision tree algorithm terminology</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953f992",
   "metadata": {},
   "source": [
    "in decision tree algo. , there is a tree like structure in which\n",
    "- each internal node represents a test on an attribute,\n",
    "- each branch represents the outcome of the test, and \n",
    "- each leaf node represents a class label.\n",
    "- the path from the root node to leaf node represent classification rule.\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>Root node</b></h5>\n",
    "it represents the entire population or sample. this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>splitting</b></h5>\n",
    "it's a process of diving a node into two or more sub-nodes\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>decision node</b></h5>\n",
    "when a sub-node splits into further sub-nodes, then it is called a decision node.\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>leaf/terminal node </b></h5>\n",
    "nodes that do not split are called leaf or terminal node\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>Pruning</b></h5>\n",
    "when we remove sub-nodes of decision node, this process is called pruning. It is the opposite of process of splitting.\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>Branch/sub-tree</b></h5>\n",
    "a sub-selection of an entire tree is called a branch or sub-tree.\n",
    "\n",
    "<h5 style = \"color:aqua\"><b>Parent and child node</b></h5>\n",
    "a node which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdc339",
   "metadata": {},
   "source": [
    "<h4>4. Decision tree algorithm intuition</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69027f2f",
   "metadata": {},
   "source": [
    "The Decision Tree algorithm intuition is as follows:-\n",
    "\n",
    "I. For each attribute in the dataset, the Decision-Tree algorithm forms a node. The most important attribute is placed at the root node.\n",
    "\n",
    "II. For evaluating the task in hand, we start at the root node and we work our way down the tree by following the corresponding node that meets our condition or decision.\n",
    "\n",
    "III. This process continues until a leaf node is reached. It contains the prediction or the outcome of the Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2524cc",
   "metadata": {},
   "source": [
    "<h4>5. Attribute selection measures</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe8b09",
   "metadata": {},
   "source": [
    "The primary challenge in the Decision Tree implementation is to identify the attributes which we consider as the root node and each level. This process is known as the <b>attributes selection</b>. There are different attributes selection measure to identify the attribute which can be considered as the root node at each level.\n",
    "\n",
    "There are 2 popular attribute selection measures. They are as follows:-\n",
    "<b>\n",
    "* Information gain\n",
    "\n",
    "* Gini index</b>\n",
    "\n",
    "While using <b>Information gain</b> as a criterion, we assume attributes to be categorical and for <b>Gini index</b> attributes are assumed to be continuous.\n",
    "\n",
    "<h5>5.1 Information gain</h5>\n",
    "By using information gain as a criterion, we try to estimate the information contained by each attribute. To understand the concept of Information Gain, we need to know another concept called <b>Entropy</b>.\n",
    "\n",
    "<h4>Entropy</h4>\n",
    "Entropy measures the impurity in the given dataset. In Physics and Mathematics, entropy is referred to as the randomness or uncertainty of a random variable X. In information theory, it refers to the impurity in a group of examples. <b>Information gain</b> is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values.\n",
    "\n",
    "Entropy is represented by the following formula:-\n",
    "\n",
    "![alt text](entropy-formula.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
